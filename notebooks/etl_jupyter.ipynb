{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cf2d77a-0ee3-4a67-bd1e-d63d5fa0c9a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example of Databricks Job Notebook for Vector Indexing using HANA Vector DB and Generative AI Hub SDK\n",
    "\n",
    "This notebook shows an example of how HANA Vector DB and Generative AI Hub SDK can be used in a Databricks Job to generate embeddings from data in Azure Data Lake Service (ADLS), and writing these embedding vectors to HANA Vector DB. The entire process is split into the Extract, Transform, Load phases in this notebook, as this is essentially a data engineering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e89e4d98-cd71-4ad2-8bf9-09fd29d4c2ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab4a30f0-51ed-4673-add9-0530d2ccb9cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Install required packages\n",
    "- `html2text`: Converting text from HTML to Markdown format\n",
    "- `generative-ai-hub-sdk`: For working with Generative AI models in GenAI Hub\n",
    "- `hdbcli`: For connection to HANA Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09667b55-83cd-4b3a-beae-da86d7fec806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install html2text \"generative-ai-hub-sdk[all]\" hdbcli\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe7670e6-8c6d-47c6-9b50-9ff294c7e4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b16c875-393e-40b3-a306-5b361875f9ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ingest raw HTML files from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b3fe342-85f5-40d3-943e-0b149da10155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "HTML_FILEPATHS_GLOB_PATTERN = \"abfss://aitm@coredatalaketestint.dfs.core.windows.net/sandbox/example_wikis/*\"\n",
    "\n",
    "html_binary_spark_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"binaryFile\")\n",
    "    .options(pathGlobFilter=\"*.html\")\n",
    "    .load(HTML_FILEPATHS_GLOB_PATTERN)\n",
    "    .withColumn(\"content\", expr(\"CAST(content AS STRING)\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa9055ca-1a8e-4750-9177-3c6026f629d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(html_binary_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d401a636-13ee-4fb2-a0a2-39a3f0965f27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffd03e0a-902d-4dd6-bdcc-f2a66114c50f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Convert wiki texts from HTML to Markdown, and clean the text contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7063c7b-dfd8-443d-a1dd-d8a1da57427b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html2text\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "AUTHORS_REGEX_PATTERN = re.compile(r\"Created\\sby.*last\\smodified.*[A-Z][a-z]{2}\\s\\d{2}\\,\\s\\d{4}\")\n",
    "\n",
    "convert_html_to_markdown = udf(html2text.html2text, StringType())\n",
    "\n",
    "@udf(StringType())\n",
    "def strip_header_and_footer(content: str) -> str:\n",
    "    return content[53:-100]\n",
    "\n",
    "@udf(StringType())\n",
    "def remove_authors(content: str) -> str:\n",
    "    return AUTHORS_REGEX_PATTERN.sub(\"\", content)\n",
    "\n",
    "@udf(StringType())\n",
    "def get_filename(path: str) -> str:\n",
    "    return path.split(\"/\")[-1]\n",
    "\n",
    "wiki_texts_spark_df = (\n",
    "    html_binary_spark_df\n",
    "    .withColumn(\"content\", convert_html_to_markdown(col(\"content\")))\n",
    "    .withColumn(\"content\", strip_header_and_footer(col(\"content\")))\n",
    "    .withColumn(\"content\", remove_authors(col(\"content\")))\n",
    "    .withColumn(\"filename\", get_filename(col(\"path\")))\n",
    "    .selectExpr(\"filename\", \"content\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab46f8cc-292a-4db6-9bee-28cb2c714bec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(wiki_texts_spark_df.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cdb3cee-17f3-4a42-825c-262fe5c7a633",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Split the wiki texts into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9766efd-0502-49b0-aefb-70e96c3e9f52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PySparkDataFrameLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "full_contents_loader = PySparkDataFrameLoader(spark_session=spark, df=wiki_texts_spark_df, page_content_column=\"content\")\n",
    "documents_to_split = full_contents_loader.load()\n",
    "\n",
    "document_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "document_chunks = document_splitter.split_documents(documents_to_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f731d8bb-9b96-445c-a063-e40b1bad4d70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cac93d17-5937-40a9-adce-8fa3374ace55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read secrets for GenAI Hub and HANA Vector DB from secret scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d071dad8-1cb0-4cc9-8f55-cfe856f82fd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SECRET_SCOPE = \"TEST_AITM_SCOPE\"\n",
    "gen_ai_hub_service_key = json.loads(dbutils.secrets.get(scope=SECRET_SCOPE, key=\"EXAMPLE_GENAI_HUB_SERVICE_KEY\"))\n",
    "hana_secrets = json.loads(dbutils.secrets.get(scope=SECRET_SCOPE, key=\"EXAMPLE_HANA_VECTOR_SECRETS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5690db84-0a7c-4c0e-b24b-ade1ff3b4db9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Initialize connection object for HANA Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803d45f6-3beb-4ac5-8542-1a8d6f3f329d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "\n",
    "hana_conn = dbapi.connect(\n",
    "    address=hana_secrets[\"host\"],\n",
    "    port=hana_secrets[\"port\"],\n",
    "    user=hana_secrets[\"user\"],\n",
    "    password=hana_secrets[\"password\"],\n",
    "    autocommit=True,\n",
    "    sslTrustStore=hana_secrets[\"certificate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "417f3f82-5f3b-417b-9a70-fe1b769152b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Initialize client for GenAI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "244e0733-65d3-4b98-a3b4-b4aba6514364",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "\n",
    "os.environ[\"AICORE_AUTH_URL\"] = gen_ai_hub_service_key[\"url\"]\n",
    "os.environ[\"AICORE_CLIENT_ID\"] = gen_ai_hub_service_key[\"clientid\"]\n",
    "os.environ[\"AICORE_CLIENT_SECRET\"] = gen_ai_hub_service_key[\"clientsecret\"]\n",
    "os.environ[\"AICORE_RESOURCE_GROUP\"] = gen_ai_hub_service_key[\"appname\"].split(\"!\")[0]\n",
    "os.environ[\"AICORE_BASE_URL\"] = f\"{gen_ai_hub_service_key['serviceurls']['AI_API_URL']}/v2\"\n",
    "\n",
    "proxy_client = get_proxy_client(\"gen-ai-hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8cb6fd-35ba-41c7-9d56-20c71c161b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Define LangChain object for embedding model, and initialize LangChain object for working with HANA Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da89a6e-c47d-4d98-8e86-fc9ec3b1b6a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "from langchain_community.vectorstores import HanaDB\n",
    "\n",
    "embeddings = init_embedding_model(\"text-embedding-ada-002\", proxy_client=proxy_client)\n",
    "hana_vectordb = HanaDB(embedding=embeddings, connection=hana_conn, table_name=\"DATABRICKS_HANA_EXAMPLE_VECTORSTORE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c19d87-ce26-4970-bdf1-c8b74d922c2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write chunks to HANA Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "817982df-f034-444e-a7ac-0525a533e2af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hana_vectordb.add_documents(document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77300153-0334-437e-a9bb-3593b089c906",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\\[Optional\\] Send query via in-built retriever object for retrieval of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5619ac89-f1b5-4e30-9b22-4ad8d421f389",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hana_vector_retriever = hana_vectordb.as_retriever()\n",
    "\n",
    "hana_vector_retriever.get_relevant_documents(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3759482-9fae-478d-a8c0-5d68ebd73a81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_jupyter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
